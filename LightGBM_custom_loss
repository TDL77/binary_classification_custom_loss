def first_grad_logreg_beta_sklearn(y, predt):
    '''Compute the first derivative for custom logloss function'''
    import numpy as np

    predt = np.where(predt >= 0,
                 1. / (1. + np.exp(-predt)),
                 np.exp(predt) / (1. + np.exp(predt)))
    return (y + beta - beta * y) * predt - y

def second_grad_logreg_beta_sklearn(y, predt):
    '''Compute the second derivative for custom logloss function'''
    import numpy as np

    predt = np.where(predt >= 0,
                 1. / (1. + np.exp(-predt)),
                 np.exp(predt) / (1. + np.exp(predt)))
    return (y + beta - beta * y) * predt * (1 - predt)

def logregobj_beta_sklearn(y, predt):
    '''Custom logloss function update'''
    grad = first_grad_logreg_beta_sklearn(y, predt)
    hess = second_grad_logreg_beta_sklearn(y, predt)
    return grad, hess

def logreg_err_beta_sklearn(predt, y):
    '''Custom evaluation metric that should be in line with custom loss function'''
    import numpy as np
    
    predt = 1.0 / (1.0 + np.exp(-predt))
    predt = np.clip(predt, 10e-7, 1-10e-7)
    loss_fn = y*np.log(predt)
    loss_fp = (1.0 - y)*np.log(1.0 - predt)
    return 'logreg_error',  np.sum(-(beta*loss_fn+loss_fp))/len(y), False
    
    
beta=.4

params = {
            'n_estimators': 500,
            'eta': 0.05,
            'disable_default_eval_metric': 1
        }   

lgb_clf = lgb.LGBMClassifier(objective=logregobj_beta_sklearn,**params)

lgb_clf.fit(X_train,y_train,eval_set=[(X_val,y_val)],
                eval_metric=logreg_err_beta_sklearn,
                early_stopping_rounds=10,
                verbose=10)

y_pred_val = lgb_clf.predict(X_val)
y_pred_train  = lgb_clf.predict(X_train)
